{
 "nbformat": 4,
 "nbformat_minor": 5,
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0",
   "mimetype": "text/x-python",
   "file_extension": ".py"
  },
  "colab": {
   "provenance": []
  }
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TSA Chapter 8: Modern Extensions - ARFIMA, ML, and LSTM\n",
    "\n",
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/QuantLet/TSA/blob/main/TSA_ch8/TSA_ch8_modern.ipynb)\n",
    "\n",
    "This notebook demonstrates:\n- Long memory and ARFIMA models, Random Forest for time series\n- LSTM neural networks and model comparison."
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "!pip install numpy pandas matplotlib scipy scikit-learn arch -q"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np",
    "import pandas as pd",
    "import matplotlib.pyplot as plt",
    "from scipy import stats",
    "from sklearn.ensemble import RandomForestRegressor",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error",
    "import warnings"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Chart style settings - Nature journal quality",
    "plt.rcParams['figure.facecolor'] = 'none'",
    "plt.rcParams['axes.facecolor'] = 'none'",
    "plt.rcParams['savefig.facecolor'] = 'none'",
    "plt.rcParams['axes.grid'] = False",
    "plt.rcParams['font.family'] = 'sans-serif'",
    "plt.rcParams['font.sans-serif'] = ['Helvetica', 'Arial', 'DejaVu Sans']",
    "plt.rcParams['font.size'] = 8",
    "plt.rcParams['axes.labelsize'] = 9",
    "plt.rcParams['axes.titlesize'] = 10",
    "plt.rcParams['xtick.labelsize'] = 8",
    "plt.rcParams['ytick.labelsize'] = 8",
    "plt.rcParams['legend.fontsize'] = 8",
    "plt.rcParams['legend.facecolor'] = 'none'",
    "plt.rcParams['legend.framealpha'] = 0",
    "plt.rcParams['axes.spines.top'] = False",
    "plt.rcParams['axes.spines.right'] = False",
    "plt.rcParams['axes.linewidth'] = 0.5",
    "plt.rcParams['lines.linewidth'] = 0.75",
    "",
    "",
    "",
    "",
    "# =============================================================================",
    "# 1. Long Memory and ARFIMA",
    "# =============================================================================",
    "",
    "",
    "",
    "",
    "",
    "",
    "# Simulate different d values",
    "",
    "",
    "",
    "",
    "",
    "",
    "plt.suptitle('ARFIMA: Effect of Fractional Differencing Parameter d', fontweight='bold', fontsize=14, y=1.02)",
    "plt.tight_layout()",
    "",
    "",
    "# =============================================================================",
    "# 2. ACF Comparison: Short vs Long Memory",
    "# =============================================================================",
    "",
    "# Short memory AR(1)",
    "",
    "# Long memory ARFIMA",
    "",
    "",
    "# ACF comparison",
    "",
    "",
    "# Log-log plot to show decay rate",
    "",
    "plt.tight_layout()",
    "",
    "# =============================================================================",
    "# 3. Feature Engineering for ML",
    "# =============================================================================",
    "",
    "# Generate time series with pattern",
    "",
    "# Create features",
    "",
    "",
    "",
    "",
    "",
    "# Visualize features",
    "",
    "",
    "",
    "",
    "",
    "plt.tight_layout()",
    "",
    "# =============================================================================",
    "# 4. Random Forest for Time Series",
    "# =============================================================================",
    "",
    "# Train/test split (time series aware)",
    "",
    "",
    "# Train Random Forest",
    "",
    "# Predictions",
    "",
    "# Metrics",
    "",
    "# Feature importance",
    "",
    "",
    "",
    "# Predictions vs Actual - show training context",
    "",
    "",
    "# Visual separator between train and test",
    "",
    "",
    "# Feature importance",
    "",
    "plt.tight_layout()",
    "",
    "# =============================================================================",
    "# 5. Time Series Cross-Validation",
    "# =============================================================================",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "plt.tight_layout()",
    "",
    "",
    "# =============================================================================",
    "# 6. LSTM Concept Visualization",
    "# =============================================================================",
    "",
    "# Simplified LSTM simulation (conceptual)",
    "",
    "",
    "",
    "",
    "",
    "# Demo sequence",
    "",
    "",
    "# Input sequence",
    "",
    "# Gates",
    "",
    "# Cell state",
    "",
    "# Hidden state",
    "",
    "# LSTM Architecture diagram (conceptual)",
    "",
    "# Advantages",
    "",
    "plt.tight_layout()",
    "",
    "# =============================================================================",
    "# 7. Model Comparison Summary",
    "# =============================================================================",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {},
   "execution_count": null,
   "outputs": [],
   "source": [
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "def save_fig(name):\n",
    "    \"\"\"Save figure with transparent background.\"\"\"\n",
    "    plt.savefig(f'{name}.pdf', bbox_inches='tight', transparent=True, dpi=300)\n",
    "    plt.savefig(f'{name}.png', bbox_inches='tight', transparent=True, dpi=300)\n",
    "    plt.close()\n",
    "    print(f\"   Saved: {name}.pdf\")\n",
    "print(\"=\" * 70)\n",
    "print(\"MODERN EXTENSIONS: ARFIMA, ML, AND LSTM\")\n",
    "print(\"=\" * 70)\n",
    "np.random.seed(42)\n",
    "print(\"\\n1. LONG MEMORY AND ARFIMA\")\n",
    "print(\"-\" * 40)\n",
    "def simulate_arfima(n, d, phi=0, theta=0, sigma=1):\n",
    "    \"\"\"Simulate ARFIMA(p, d, q) process using fractional differencing.\"\"\"\n",
    "    # Generate white noise\n",
    "    eps = np.random.normal(0, sigma, n + 1000)\n",
    "    # Fractional integration weights (truncated)\n",
    "    k = np.arange(1000)\n",
    "    weights = np.zeros(1000)\n",
    "    weights[0] = 1\n",
    "    for j in range(1, 1000):\n",
    "        weights[j] = weights[j-1] * (d + j - 1) / j\n",
    "    # Apply fractional integration\n",
    "    y = np.convolve(eps, weights, mode='full')[:n+1000]\n",
    "    # Apply AR if phi != 0\n",
    "    if phi != 0:\n",
    "        for t in range(1, len(y)):\n",
    "            y[t] = y[t] + phi * y[t-1]\n",
    "    return y[-n:]\n",
    "n = 500\n",
    "d_values = [0, 0.2, 0.4]\n",
    "colors = ['#1A3A6E', '#2E7D32', '#DC3545']\n",
    "fig, axes = plt.subplots(2, 3, figsize=(14, 8))\n",
    "for idx, (d, color) in enumerate(zip(d_values, colors)):\n",
    "    y = simulate_arfima(n, d)\n",
    "    # Time series\n",
    "    axes[0, idx].plot(y, color=color, linewidth=0.8, label=f'd = {d}')\n",
    "    axes[0, idx].set_title(f'ARFIMA(0, {d}, 0)', fontweight='bold')\n",
    "    axes[0, idx].set_xlabel('Time')\n",
    "    axes[0, idx].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "    # ACF (manual calculation)\n",
    "    acf_vals = [1.0]\n",
    "    for lag in range(1, 31):\n",
    "        acf_vals.append(np.corrcoef(y[lag:], y[:-lag])[0, 1])\n",
    "    axes[1, idx].bar(range(31), acf_vals, color=color, alpha=0.7, edgecolor='white')\n",
    "    axes[1, idx].axhline(y=1.96/np.sqrt(n), color='red', linestyle='--', linewidth=1)\n",
    "    axes[1, idx].axhline(y=-1.96/np.sqrt(n), color='red', linestyle='--', linewidth=1)\n",
    "    axes[1, idx].set_title(f'ACF: d = {d}', fontweight='bold')\n",
    "    axes[1, idx].set_xlabel('Lag')\n",
    "save_fig('ch8_arfima')\n",
    "print(\"   ARFIMA(p, d, q) with 0 < d < 0.5:\")\n",
    "print(\"   - Long memory: slow hyperbolic ACF decay\")\n",
    "print(\"   - Stationary but with persistent dependence\")\n",
    "print(\"   - ACF decays like k^(2d-1)\")\n",
    "print(\"\\n2. SHORT VS LONG MEMORY\")\n",
    "print(\"-\" * 40)\n",
    "ar1 = np.zeros(n)\n",
    "for t in range(1, n):\n",
    "    ar1[t] = 0.7 * ar1[t-1] + np.random.normal(0, 1)\n",
    "arfima = simulate_arfima(n, d=0.35)\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "lags = 50\n",
    "acf_ar1 = [np.corrcoef(ar1[lag:], ar1[:-lag])[0, 1] for lag in range(1, lags+1)]\n",
    "acf_arfima = [np.corrcoef(arfima[lag:], arfima[:-lag])[0, 1] for lag in range(1, lags+1)]\n",
    "axes[0].plot(range(1, lags+1), acf_ar1, color='#1A3A6E', linewidth=2, marker='o', markersize=3, label='AR(1) \u03c6=0.7')\n",
    "axes[0].plot(range(1, lags+1), acf_arfima, color='#DC3545', linewidth=2, marker='s', markersize=3, label='ARFIMA d=0.35')\n",
    "axes[0].axhline(y=0, color='gray', linestyle='--', linewidth=0.5)\n",
    "axes[0].set_xlabel('Lag')\n",
    "axes[0].set_ylabel('ACF')\n",
    "axes[0].set_title('ACF: Short Memory vs Long Memory', fontweight='bold')\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n",
    "axes[1].loglog(range(1, lags+1), np.abs(acf_ar1), color='#1A3A6E', linewidth=2, marker='o', markersize=3, label='AR(1): Exponential decay')\n",
    "axes[1].loglog(range(1, lags+1), np.abs(acf_arfima), color='#DC3545', linewidth=2, marker='s', markersize=3, label='ARFIMA: Hyperbolic decay')\n",
    "axes[1].set_xlabel('Lag (log scale)')\n",
    "axes[1].set_ylabel('|ACF| (log scale)')\n",
    "axes[1].set_title('Log-Log Plot: Decay Comparison', fontweight='bold')\n",
    "axes[1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n",
    "save_fig('ch8_memory_comparison')\n",
    "print(\"\\n3. FEATURE ENGINEERING FOR ML\")\n",
    "print(\"-\" * 40)\n",
    "n_ml = 500\n",
    "t = np.arange(n_ml)\n",
    "trend = 0.01 * t\n",
    "seasonality = 5 * np.sin(2 * np.pi * t / 30)\n",
    "y_ml = trend + seasonality + np.random.normal(0, 1, n_ml)\n",
    "def create_features(y, lags=5):\n",
    "    \"\"\"Create lag features and rolling statistics.\"\"\"\n",
    "    df = pd.DataFrame({'y': y})\n",
    "    # Lag features\n",
    "    for i in range(1, lags + 1):\n",
    "        df[f'lag_{i}'] = df['y'].shift(i)\n",
    "    # Rolling features\n",
    "    df['rolling_mean_5'] = df['y'].rolling(window=5).mean().shift(1)\n",
    "    df['rolling_std_5'] = df['y'].rolling(window=5).std().shift(1)\n",
    "    df['rolling_mean_10'] = df['y'].rolling(window=10).mean().shift(1)\n",
    "    return df.dropna()\n",
    "df_features = create_features(y_ml, lags=5)\n",
    "print(f\"   Features created: {list(df_features.columns[1:])}\")\n",
    "fig, axes = plt.subplots(2, 2, figsize=(12, 8))\n",
    "axes[0, 0].plot(y_ml, color='#1A3A6E', linewidth=0.8, label='Original series')\n",
    "axes[0, 0].set_title('Original Time Series', fontweight='bold')\n",
    "axes[0, 0].set_xlabel('Time')\n",
    "axes[0, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "axes[0, 1].scatter(df_features['lag_1'], df_features['y'], color='#1A3A6E', alpha=0.5, s=10, label='Y\u209c vs Y\u209c\u208b\u2081')\n",
    "axes[0, 1].set_xlabel('Y\u209c\u208b\u2081')\n",
    "axes[0, 1].set_ylabel('Y\u209c')\n",
    "axes[0, 1].set_title('Lag 1 Feature', fontweight='bold')\n",
    "axes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "axes[1, 0].plot(df_features['rolling_mean_5'].values, color='#DC3545', linewidth=1.5, label='Rolling Mean (5)')\n",
    "axes[1, 0].plot(df_features['y'].values, color='#1A3A6E', linewidth=0.5, alpha=0.5, label='Original')\n",
    "axes[1, 0].set_title('Rolling Mean Feature', fontweight='bold')\n",
    "axes[1, 0].set_xlabel('Time')\n",
    "axes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n",
    "axes[1, 1].plot(df_features['rolling_std_5'].values, color='#2E7D32', linewidth=1.5, label='Rolling Std (5)')\n",
    "axes[1, 1].set_title('Rolling Std Feature (Volatility)', fontweight='bold')\n",
    "axes[1, 1].set_xlabel('Time')\n",
    "axes[1, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "save_fig('ch8_features')\n",
    "print(\"\\n4. RANDOM FOREST FOR TIME SERIES\")\n",
    "print(\"-\" * 40)\n",
    "train_size = int(len(df_features) * 0.8)\n",
    "train = df_features.iloc[:train_size]\n",
    "test = df_features.iloc[train_size:]\n",
    "X_train = train.drop('y', axis=1)\n",
    "y_train = train['y']\n",
    "X_test = test.drop('y', axis=1)\n",
    "y_test = test['y']\n",
    "rf_model = RandomForestRegressor(n_estimators=100, max_depth=10, random_state=42)\n",
    "rf_model.fit(X_train, y_train)\n",
    "y_pred = rf_model.predict(X_test)\n",
    "rmse = np.sqrt(mean_squared_error(y_test, y_pred))\n",
    "mae = mean_absolute_error(y_test, y_pred)\n",
    "print(f\"   Random Forest Performance:\")\n",
    "print(f\"   RMSE: {rmse:.4f}\")\n",
    "print(f\"   MAE: {mae:.4f}\")\n",
    "feature_importance = pd.DataFrame({\n",
    "    'feature': X_train.columns,\n",
    "    'importance': rf_model.feature_importances_\n",
    "}).sort_values('importance', ascending=False)\n",
    "print(f\"\\n   Top 5 Important Features:\")\n",
    "for _, row in feature_importance.head().iterrows():\n",
    "    print(f\"   - {row['feature']}: {row['importance']:.4f}\")\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 4))\n",
    "train_plot_size = min(50, len(y_train))  # Show last 50 training points for context\n",
    "combined_actual = np.concatenate([y_train.values[-train_plot_size:], y_test.values])\n",
    "combined_pred = np.concatenate([np.full(train_plot_size, np.nan), y_pred])\n",
    "time_idx = np.arange(len(combined_actual))\n",
    "split_idx = train_plot_size\n",
    "axes[0].plot(time_idx[:split_idx], combined_actual[:split_idx], color='#1A3A6E', linewidth=1.5, label='Training')\n",
    "axes[0].plot(time_idx[split_idx:], combined_actual[split_idx:], color='#2E7D32', linewidth=1.5, label='Test (Actual)')\n",
    "axes[0].plot(time_idx[split_idx:], y_pred, color='#DC3545', linewidth=1.5, linestyle='--', label='RF Predicted')\n",
    "axes[0].axvline(x=split_idx, color='black', linestyle='--', linewidth=1.5, alpha=0.7)\n",
    "y_pos = axes[0].get_ylim()[1] - 0.05 * (axes[0].get_ylim()[1] - axes[0].get_ylim()[0])\n",
    "axes[0].text(split_idx, y_pos, '  Test ', fontsize=9, ha='left', va='top',\n",
    "             color='black', fontweight='bold', alpha=0.8)\n",
    "axes[0].set_title('Random Forest: Actual vs Predicted', fontweight='bold')\n",
    "axes[0].set_xlabel('Time')\n",
    "axes[0].set_ylabel('Value')\n",
    "axes[0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "colors_fi = ['#1A3A6E' if i < 3 else '#CCCCCC' for i in range(len(feature_importance))]\n",
    "axes[1].barh(feature_importance['feature'], feature_importance['importance'], color=colors_fi)\n",
    "axes[1].set_xlabel('Importance')\n",
    "axes[1].set_title('Feature Importance', fontweight='bold')\n",
    "axes[1].invert_yaxis()\n",
    "save_fig('ch8_random_forest')\n",
    "print(\"\\n5. TIME SERIES CROSS-VALIDATION\")\n",
    "print(\"-\" * 40)\n",
    "fig, ax = plt.subplots(figsize=(12, 5))\n",
    "n_splits = 5\n",
    "fold_size = len(df_features) // (n_splits + 1)\n",
    "colors_cv = plt.cm.Blues(np.linspace(0.3, 0.9, n_splits))\n",
    "for i in range(n_splits):\n",
    "    train_end = (i + 2) * fold_size\n",
    "    test_start = train_end\n",
    "    test_end = test_start + fold_size\n",
    "    # Training data\n",
    "    ax.barh(i, train_end, height=0.4, color=colors_cv[i], alpha=0.7, label='Train' if i == 0 else '')\n",
    "    # Test data\n",
    "    ax.barh(i, test_end - test_start, left=test_start, height=0.4, color='#DC3545', alpha=0.7, label='Test' if i == 0 else '')\n",
    "ax.set_xlabel('Time Index')\n",
    "ax.set_ylabel('Fold')\n",
    "ax.set_yticks(range(n_splits))\n",
    "ax.set_yticklabels([f'Fold {i+1}' for i in range(n_splits)])\n",
    "ax.set_title('Time Series Cross-Validation (Expanding Window)', fontweight='bold')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=2, frameon=False)\n",
    "save_fig('ch8_cross_validation')\n",
    "print(\"   Time series CV rules:\")\n",
    "print(\"   - Never use future data for training\")\n",
    "print(\"   - Expanding or sliding window approach\")\n",
    "print(\"   - Maintain temporal order\")\n",
    "print(\"\\n6. LSTM ARCHITECTURE\")\n",
    "print(\"-\" * 40)\n",
    "def simple_lstm_demo(sequence, hidden_size=10):\n",
    "    \"\"\"Simplified LSTM demonstration.\"\"\"\n",
    "    n = len(sequence)\n",
    "    # Simulated cell state and hidden state evolution\n",
    "    cell_state = np.zeros(n)\n",
    "    hidden_state = np.zeros(n)\n",
    "    # Simulate gates (simplified)\n",
    "    forget_gate = 1 / (1 + np.exp(-0.5 * sequence))  # sigmoid\n",
    "    input_gate = 1 / (1 + np.exp(-0.3 * sequence))\n",
    "    output_gate = 1 / (1 + np.exp(-0.4 * sequence))\n",
    "    for t in range(1, n):\n",
    "        # Simplified update\n",
    "        cell_state[t] = forget_gate[t] * cell_state[t-1] + input_gate[t] * np.tanh(sequence[t])\n",
    "        hidden_state[t] = output_gate[t] * np.tanh(cell_state[t])\n",
    "    return cell_state, hidden_state, forget_gate, input_gate, output_gate\n",
    "demo_seq = np.sin(np.linspace(0, 4*np.pi, 100)) + 0.2 * np.random.randn(100)\n",
    "cell, hidden, fg, ig, og = simple_lstm_demo(demo_seq)\n",
    "fig, axes = plt.subplots(3, 2, figsize=(12, 10))\n",
    "axes[0, 0].plot(demo_seq, color='#1A3A6E', linewidth=1.5, label='Input X\u209c')\n",
    "axes[0, 0].set_title('Input Sequence', fontweight='bold')\n",
    "axes[0, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "axes[0, 1].plot(fg, color='#DC3545', linewidth=1.5, alpha=0.8, label='Forget Gate')\n",
    "axes[0, 1].plot(ig, color='#2E7D32', linewidth=1.5, alpha=0.8, label='Input Gate')\n",
    "axes[0, 1].plot(og, color='#E67E22', linewidth=1.5, alpha=0.8, label='Output Gate')\n",
    "axes[0, 1].set_title('LSTM Gates', fontweight='bold')\n",
    "axes[0, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=3, frameon=False)\n",
    "axes[1, 0].plot(cell, color='#9B59B6', linewidth=2, label='Cell State C\u209c')\n",
    "axes[1, 0].set_title('Cell State (Long-term Memory)', fontweight='bold')\n",
    "axes[1, 0].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "axes[1, 1].plot(hidden, color='#1ABC9C', linewidth=2, label='Hidden State h\u209c')\n",
    "axes[1, 1].set_title('Hidden State (Short-term Memory)', fontweight='bold')\n",
    "axes[1, 1].legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=1, frameon=False)\n",
    "axes[2, 0].text(0.5, 0.9, 'LSTM Cell Architecture', fontsize=14, fontweight='bold',\n",
    "               ha='center', transform=axes[2, 0].transAxes)\n",
    "axes[2, 0].text(0.5, 0.7, 'Forget Gate: f\u209c = \u03c3(Wf\u00b7[h\u209c\u208b\u2081, x\u209c] + bf)', fontsize=10,\n",
    "               ha='center', transform=axes[2, 0].transAxes, family='monospace')\n",
    "axes[2, 0].text(0.5, 0.55, 'Input Gate: i\u209c = \u03c3(Wi\u00b7[h\u209c\u208b\u2081, x\u209c] + bi)', fontsize=10,\n",
    "               ha='center', transform=axes[2, 0].transAxes, family='monospace')\n",
    "axes[2, 0].text(0.5, 0.4, 'Cell State: C\u209c = f\u209c\u2299C\u209c\u208b\u2081 + i\u209c\u2299tanh(Wc\u00b7[h\u209c\u208b\u2081, x\u209c])', fontsize=10,\n",
    "               ha='center', transform=axes[2, 0].transAxes, family='monospace')\n",
    "axes[2, 0].text(0.5, 0.25, 'Output Gate: o\u209c = \u03c3(Wo\u00b7[h\u209c\u208b\u2081, x\u209c] + bo)', fontsize=10,\n",
    "               ha='center', transform=axes[2, 0].transAxes, family='monospace')\n",
    "axes[2, 0].text(0.5, 0.1, 'Hidden State: h\u209c = o\u209c\u2299tanh(C\u209c)', fontsize=10,\n",
    "               ha='center', transform=axes[2, 0].transAxes, family='monospace')\n",
    "axes[2, 0].axis('off')\n",
    "axes[2, 1].text(0.5, 0.9, 'LSTM Advantages for Time Series', fontsize=14, fontweight='bold',\n",
    "               ha='center', transform=axes[2, 1].transAxes)\n",
    "advantages = [\n",
    "    '\u2713 Captures long-term dependencies',\n",
    "    '\u2713 Handles variable-length sequences',\n",
    "    '\u2713 Mitigates vanishing gradient problem',\n",
    "    '\u2713 Learns complex nonlinear patterns',\n",
    "    '\u2713 Automatic feature learning'\n",
    "]\n",
    "for i, adv in enumerate(advantages):\n",
    "    axes[2, 1].text(0.1, 0.7 - i*0.12, adv, fontsize=11,\n",
    "                   transform=axes[2, 1].transAxes)\n",
    "axes[2, 1].axis('off')\n",
    "save_fig('ch8_lstm')\n",
    "print(\"\\n7. MODEL COMPARISON SUMMARY\")\n",
    "print(\"-\" * 40)\n",
    "comparison = pd.DataFrame({\n",
    "    'Model': ['ARIMA', 'ARFIMA', 'Random Forest', 'LSTM'],\n",
    "    'Complexity': ['Low', 'Medium', 'Medium', 'High'],\n",
    "    'Interpretability': ['High', 'High', 'Medium', 'Low'],\n",
    "    'Long Memory': ['No', 'Yes', 'With features', 'Yes'],\n",
    "    'Nonlinearity': ['No', 'No', 'Yes', 'Yes'],\n",
    "    'Data Required': ['Small', 'Medium', 'Medium', 'Large']\n",
    "})\n",
    "print(\"\\n   Model Comparison:\")\n",
    "print(comparison.to_string(index=False))\n",
    "fig, ax = plt.subplots(figsize=(10, 5))\n",
    "models = ['ARIMA', 'ARFIMA', 'Random Forest', 'LSTM']\n",
    "metrics = ['Interpretability', 'Flexibility', 'Data Efficiency', 'Long Memory']\n",
    "scores = np.array([\n",
    "    [5, 2, 5, 1],  # ARIMA\n",
    "    [4, 3, 4, 5],  # ARFIMA\n",
    "    [3, 4, 3, 3],  # RF\n",
    "    [1, 5, 1, 5]   # LSTM\n",
    "])\n",
    "x = np.arange(len(metrics))\n",
    "width = 0.2\n",
    "colors_bar = ['#1A3A6E', '#2E7D32', '#DC3545', '#E67E22']\n",
    "for i, (model, color) in enumerate(zip(models, colors_bar)):\n",
    "    ax.bar(x + i*width, scores[i], width, label=model, color=color, alpha=0.8)\n",
    "ax.set_ylabel('Score (1-5)')\n",
    "ax.set_xticks(x + width * 1.5)\n",
    "ax.set_xticklabels(metrics)\n",
    "ax.set_title('Model Comparison by Characteristics', fontweight='bold')\n",
    "ax.legend(loc='upper center', bbox_to_anchor=(0.5, -0.15), ncol=4, frameon=False)\n",
    "ax.set_ylim(0, 6)\n",
    "save_fig('ch8_model_comparison')\n",
    "print(\"\\n\" + \"=\" * 70)\n",
    "print(\"MODERN EXTENSIONS ANALYSIS COMPLETE\")\n",
    "print(\"=\" * 70)\n",
    "print(\"\\nOutput files:\")\n",
    "print(\"  - ch8_arfima.pdf: ARFIMA long memory demonstration\")\n",
    "print(\"  - ch8_memory_comparison.pdf: Short vs long memory\")\n",
    "print(\"  - ch8_features.pdf: Feature engineering\")\n",
    "print(\"  - ch8_random_forest.pdf: Random Forest predictions\")\n",
    "print(\"  - ch8_cross_validation.pdf: Time series CV\")\n",
    "print(\"  - ch8_lstm.pdf: LSTM architecture\")\n",
    "print(\"  - ch8_model_comparison.pdf: Model comparison\")"
   ]
  }
 ]
}